{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Structured Self-attentive Sentence Embedding\n",
    "\n",
    "After the novelty of word embeddings to create new numerical representations of words, natural language processing (NLP) has still been effectively improved in many ways. Along with the widespread use of embedding techniques, many other methods have been developed to further express the semantics and meanings of sentences with words:\n",
    "\n",
    "1. A vector representation of multiple words in a sentence can be concatenated or weighted to obtain a vector to represent the entirety of a sentence.\n",
    "\n",
    "2. Convolution (CNN) and maximum pooling (MaxPooling) on the matrix of all the word vectors of the sentence, using the final result of these techniques to represent the sentence as a whole.\n",
    "\n",
    "3. Unrolling the sentence according to the time step of the word, inputting the vector representation of each word into a recurrent neural network (RNN), and using the output of the last time step of the RNN as the representation of the sentence.\n",
    "\n",
    "The above methods solve the problem of sentence meaning, but only to a certain extent. When concatenating is used in method one, if the word of the sentence is too long and the vector dimension of the word is slightly larger, then the vector dimension of the sentence will be particularly large, and the internal interaction between the words of the sentence can not be taken into account. The use of weighted averaging is not accurate and does not adequately express the impact of each word on sentence semantics.\n",
    "\n",
    "In the second method, many useful word meanings may be lost using CNNs and MaxPooling.\n",
    "\n",
    "In the third method, the representation selected is only the output of the last step. If a sentence is too long, the output of the last step does not accurately express the entirety of the sentence's semantics.\n",
    "\n",
    "Based on the aforementioned method, Zhouhan Lin, Minwei Feng et al. published a paper [A Structured Self-attentive Sentence Embedding](https://arxiv.org/pdf/1703.03130.pdf)[1] in 2017, proposing a novel method based on self-attention structures for sentence embedding and application to users' review classification, textual entailment and other NLP tasks. In the end, better results were obtained than the previous methods.\n",
    "\n",
    "In this tutorial, we will use [GluonNLP](https://gluon-nlp.mxnet.io/index.html) to reproduce the model structure in \"A Structured Self-attentive Sentence Embedding\" and apply it to [Yelp Data's review star rating data set](https://www.yelp.com/dataset/challenge) for classification.\n",
    "\n",
    "## Importing necessary packages\n",
    "\n",
    "The first step, as in every one of these tutorials, is to import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import multiprocessing as mp\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import autograd, gluon, nd\n",
    "\n",
    "# iUse sklearn's metric function to evaluate the results of the experiment\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# fixed random number seed\n",
    "np.random.seed(2018)\n",
    "mx.random.seed(2018)\n",
    "\n",
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "The next step is to load and format the data according to the requirements of our model. The dataset used in this tutorial is the Yelp users' review dataset.\n",
    "\n",
    "### Loading the dataset\n",
    "\n",
    "The [Yelp users' review dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset) is formatted as a JSON. The original paper selected 500,000 documents as the training set, 2,000 as the validation set, and 2,000 as the test set. For easier reproducibility of the experiment, we subsampled 198,000 documents from this dataset as the training set and 2,000 documents as validation set.\n",
    "\n",
    "Each sample in the data consists of a user's comment, in English, with each comment marked one through five, each number representing one of five different emotions the user expressed. Here we download, unzip, and reformat the dataset for ease of use further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading yelp_review_subset-167bb781.zip from http://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/yelp_review_subset-167bb781.zip...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(198000, 2000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the data from the server\n",
    "data_url = 'http://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/yelp_review_subset-167bb781.zip'\n",
    "zip_path = mx.gluon.utils.download(data_url)\n",
    "\n",
    "# Unzip the zip file\n",
    "zip_file = zipfile.ZipFile(zip_path)\n",
    "json_path = zip_file.extract(zip_file.namelist()[0])\n",
    "\n",
    "## Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as fr:\n",
    "    data = json.load(fr)\n",
    "\n",
    "# Create a list of review a label pairs\n",
    "dataset = [[text, int(label)] for text, label in zip(data['texts'], data['labels'])]\n",
    "\n",
    "# Randomly divide one percent from the training set as a verification set\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, 0.01)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary processing of the data\n",
    "\n",
    "The purpose of the following code is to process the raw data so that the pre-processed data can be used for model training and prediction. We will use the `SpacyTokenizer` to split the document into tokens, `ClipSequence` to crop the comments to the specified length, and then build a vocabulary based on the word frequency of the training data. Next, we attach the [Glove](https://nlp.stanford.edu/pubs/glove.pdf) [2] pre-trained word vector to the vocabulary and convert each token into the corresponding word index in the vocabulary.\n",
    "Finally, we get the standardized training data set and verification data set. Here we also define a few helper functions for later. We take advantage of the `mp.Pool()` function to spread the pre-processing over multiple cores or machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=22.94s, #Sentences=198000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=0.61s, #Sentences=2000\n"
     ]
    }
   ],
   "source": [
    "# The tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "# `length_clip` takes as input a list and outputs a list with maximum length 100.\n",
    "length_clip = nlp.data.ClipSequence(100)\n",
    "\n",
    "def preprocess(x):\n",
    "\n",
    "    # Convert the number of stars 1, 2, 3, 4, 5 to zero-based index, 0, 1, 2, 3, 4\n",
    "    data, label = x[0], x[1]-1\n",
    "\n",
    "    # Clip the length of review words\n",
    "    data = length_clip(tokenizer(data))\n",
    "    return data, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "valid_dataset, valid_data_lengths = preprocess_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates the `vocab` object and converts the dataset's words to the Glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding file glove.6B.300d.npz is not found. Downloading from Gluon Repository. This may take some time.\n",
      "Downloading /root/.mxnet/embedding/glove/glove.6B.300d.npz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/embeddings/glove/glove.6B.300d.npz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=10004, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "# Create the vocab\n",
    "train_seqs = [sample[0] for sample in train_dataset]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=10000)\n",
    "\n",
    "# Load the pre-trained embedding, in this case the Glove embedding of 300 dimensions\n",
    "embedding_weights = nlp.embedding.GloVe(source='glove.6B.300d')\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_dataset)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing, mini-batches, and the `DataLoader`\n",
    "Since each sentence may have a different length, we need to use `Pad` to fill the sentences in a mini-batch to equal lengths so that the data can be quickly tensored on the GPU. At the same time, we need to use `Stack` to stack the category tags of a batch of data. For convenience, we use `Tuple` to combine `Pad` and `Stack`.\n",
    "\n",
    "In order to make the length of the sentence pad in each mini-batch as small as possible, we should make the sentences with similar lengths in a batch as much as possible. In light of this, we consider constructing a sampler using `FixedBucketSampler`, which defines how the samples in a dataset will be iterated in a more economical way.\n",
    "\n",
    "Finally, we use `DataLoader` to build a data loader for the training and validation datasets. The training dataset requires a `FixedBucketSampler`, but the validation dataset doesn't require the sampler.\n",
    "\n",
    "Here we define the helper functions to do all of the above as well as define the hyperparameters for this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=198000, batch_num=2922\n",
      "  key=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "  cnt=[269, 4489, 13825, 14963, 14998, 14055, 12909, 11664, 10275, 100553]\n",
      "  batch_size=[320, 160, 106, 80, 64, 64, 64, 64, 64, 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gluon-nlp/src/gluonnlp/data/batchify/batchify.py:228: UserWarning: Padding value is not given and will be set automatically to 0 in data.batchify.Pad(). Please check whether this is intended (e.g. value of padding index in the vocabulary).\n",
      "  'Padding value is not given and will be set automatically to 0 '\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0),\n",
    "        nlp.data.batchify.Stack())\n",
    "\n",
    "    # In this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # Training set DataLoader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # Validation set DataLoader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the model and outlining the model's structure\n",
    "\n",
    "In the original paper, the representation of the sentence is broken into the following steps:\n",
    "\n",
    "Firstly, the sentence is disassembled into a list corresponding to the word.\n",
    "Then the words are unrolled in order, and the word vector of each word is calculated as the input of each step of the [bidirectional LSTM neural network layer](https://www.bioinf.jku.at/publications/older/2604.pdf) [3].\n",
    "Taking the output of each step of the bidirectional LSTM network layer, a matrix H is obtained. Suppose the hidden_dim of the bidirectional LSTM is `U`, the word length of the sentence is `N`, then the dimension of the last H is `N x 2U`.  For example, the sentence \"This movie is amazing!\" would be represented as:\n",
    "![](Bi-LSTM-Rep.png)\n",
    "\n",
    "Attention is very similar to when we are actually looking at an object, we always give different importance (or weights) to things in the scope of the perspective. A brief quote from skymind.ai summarizes what attention means in our daily lives as well as in neural networks in a few clear words:\n",
    "\n",
    "> The word describes the mind’s ability to allocate consideration unevenly across a field of sensation, thought and proprioception, to focus and bring certain inputs to the fore, while ignoring or diminishing the importance of others. So for neural networks, we’re basically talking about credit assignment. [4]\n",
    "\n",
    "For example, when we are communicating with people, our eyes will always pay more attention to the face of the communicator, rather than the type of trousers they are wearing or their toe nail polish. So when we are expressing a sentence with this model, we can pay different amounts of attention to the output H of the bi-directional LSTM layer.\n",
    "![](attention-nlp.png)\n",
    "$$\n",
    "A = Softmax(W_{s2}tanh(W_{s1}H^T))\n",
    "$$\n",
    "\n",
    "Here, W<sub>s1</sub> is a weight matrix with the shape: d<sub>a</sub>-by-2u, where d<sub>a</sub> is a hyperparameter.\n",
    "W<sub>s2</sub> is a weight matrix with the shape: r-by-d<sub>a</sub>, where r is the number of different attentions you want to use.\n",
    "\n",
    "When the attention matrix `A` and the output `H` of the LSTM are obtained, the final representation is obtained by $$M = AH$$.\n",
    "\n",
    "We can first customize a layer of attention, specify the number of hidden nodes (`att_unit`) and the number of attention channels (`att_hops`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom attention layer\n",
    "class SelfAttention(nn.HybridBlock):\n",
    "    def __init__(self, att_unit, att_hops, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.ut_dense = nn.Dense(att_unit, activation='tanh', flatten=False)\n",
    "            self.et_dense = nn.Dense(att_hops, activation=None, flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        # x shape: [batch_size, seq_len, embedding_width]\n",
    "        # ut shape: [batch_size, seq_len, att_unit]\n",
    "        ut = self.ut_dense(x)\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        att = F.softmax(F.transpose(et, axes=(0, 2, 1)), axis=-1)\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = F.batch_dot(att, x)\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of samples for labels are very unbalanced, applying different weights on different labels may improve the performance of the model significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the basic model characteristics in a self-attentive bi-LSTM model, and configure the layers and dropout, as well as how the model feeds forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentiveBiLSTM(nn.HybridBlock):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, natt_unit, natt_hops, nfc, nclass,\n",
    "                 drop_prob, pool_way, prune_p=None, prune_q=None, **kwargs):\n",
    "        super(SelfAttentiveBiLSTM, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            self.bilstm = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, bidirectional=True)\n",
    "            self.att_encoder = SelfAttention(natt_unit, natt_hops)\n",
    "            self.dense = nn.Dense(nfc, activation='tanh')\n",
    "            self.output_layer = nn.Dense(nclass)\n",
    "\n",
    "            self.dense_p, self.dense_q = None, None\n",
    "            if all([prune_p, prune_q]):\n",
    "                self.dense_p = nn.Dense(prune_p, activation='tanh', flatten=False)\n",
    "                self.dense_q = nn.Dense(prune_q, activation='tanh', flatten=False)\n",
    "\n",
    "            self.drop_prob = drop_prob\n",
    "            self.pool_way = pool_way\n",
    "\n",
    "    def hybrid_forward(self, F, inp):\n",
    "        # input_embed: [batch, len, emsize]\n",
    "        inp_embed = self.embedding_layer(inp)\n",
    "        h_output = self.bilstm(F.transpose(inp_embed, axes=(1, 0, 2)))\n",
    "        # att_output: [batch, att_hops, emsize]\n",
    "        att_output, att = self.att_encoder(F.transpose(h_output, axes=(1, 0, 2)))\n",
    "\n",
    "        dense_input = None\n",
    "        if self.pool_way == 'flatten':\n",
    "            dense_input = F.Dropout(F.flatten(att_output), self.drop_prob)\n",
    "        elif self.pool_way == 'mean':\n",
    "            dense_input = F.Dropout(F.mean(att_output, axis=1), self.drop_prob)\n",
    "        elif self.pool_way == 'prune' and all([self.dense_p, self.dense_q]):\n",
    "            # p_section: [batch, att_hops, prune_p]\n",
    "            p_section = self.dense_p(att_output)\n",
    "            # q_section: [batch, emsize, prune_q]\n",
    "            q_section = self.dense_q(F.transpose(att_output, axes=(0, 2, 1)))\n",
    "            dense_input = F.Dropout(F.concat(F.flatten(p_section), F.flatten(q_section), dim=-1), self.drop_prob)\n",
    "\n",
    "        dense_out = self.dense(dense_input)\n",
    "        output = self.output_layer(F.Dropout(dense_out, self.drop_prob))\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the parameters and assembling the model\n",
    "\n",
    "The resulting `M` is a matrix, and the way to classify this matrix is `flatten`-ing, `mean`-ing or `prune`-ing. Pruning is an effective way of trimming parameters that was proposed in the original paper, and has been implemented for our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300         # word embedding size\n",
    "nhidden = 300        # lstm hidden_dim\n",
    "nlayers = 2          # lstm layers\n",
    "natt_unit = 300      # the hidden_units of attention layer\n",
    "natt_hops = 2        # the channels of attention\n",
    "nfc = 512\n",
    "nclass = 5\n",
    "\n",
    "drop_prob = 0.5\n",
    "pool_way = 'flatten'  # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "model = SelfAttentiveBiLSTM(vocab_len, emsize, nhidden, nlayers,\n",
    "                            natt_unit, natt_hops, nfc, nclass,\n",
    "                            drop_prob, pool_way, prune_p, prune_q)\n",
    "\n",
    "model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "# Attach a pre-trained glove word vector to the embedding layer\n",
    "model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "# fixed the layer\n",
    "model.embedding_layer.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using r attention can improve the representation of sentences with different semantics, but if the value of each line in the attention matrix `A` (r-byn) is very close, that is, there is no difference between several attentions. Subsequently, in $$M = AH$$, the resulting `M` will contain a lot of redundant information.\n",
    "So in order to solve this problem, we should try to force `A` to ensure that the value of each line has obvious differences, that is, try to satisfy the diversity of attention. Therefore, a penalty can be used to achieve this goal.\n",
    "\n",
    "$$ P = ||(AA^T-I)||_F^2 $$\n",
    "\n",
    "\n",
    "It can be seen from the above formula that if the value of each row of `A` is more similar, the result of `P` will be larger, and the value of `A` is less similar for each row, and `P` is smaller. This means that when the r-focused diversity of `A` is larger, the smaller `P` is. So by including this penalty item with the loss of the model, we can try to ensure the diversity of `A`.\n",
    "\n",
    "We incorporate these findings in the code below adding in the penalty coefficient along with the standard loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, y, model, loss, class_weight, penal_coeff):\n",
    "    pred, att = model(x)\n",
    "    if loss_name == 'sce':\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "\n",
    "    # penalty\n",
    "    diversity_penalty = nd.batch_dot(att, nd.transpose(att, axes=(0, 2, 1))\n",
    "                        ) - nd.eye(att.shape[1], ctx=att.context)\n",
    "    l = l + penal_coeff * diversity_penalty.norm(axis=(1, 2))\n",
    "\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define what one epoch of training would be for the model for easier use later. In addition, we calculate loss, the F1 score, and accuracy for each epoch and print them for easier understanding. Additionally, we dynamically adjust the learning rate as the number of epochs increase. We also include an `is_train` boolean to allow us to know whether or not we should be altering the original model or just reporting the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch,\n",
    "              penal_coeff=0.0, clip=None, class_weight=None, loss_name='wsce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x, batch_y in data_iter:\n",
    "        batch_x = batch_x.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x, batch_y, model, loss, class_weight, penal_coeff)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x, batch_y, model, loss, class_weight, penal_coeff)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "\n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 2 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we include a helper method `train_valid` which combines the one epoch for the training data as well as the validation data, using the `is_train` boolean to swap between the two modes we discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs,\n",
    "                penal_coeff=0.0, clip=None, class_weight=None, loss_name='wsce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that we are actually training the model, we use `WeightedSoftmaxCE` to alleviate the problem of data categorical imbalance. We perform statistical analysis on the data in advance to retrieve a set of `class_weight`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "loss_name = 'wsce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "penal_coeff = 0.1\n",
    "clip = 0.5\n",
    "nepochs = 4\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array([3.0, 5.3, 4.0, 2.0, 1.0], ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've simplified our lives earlier by creating the necessary helper methods so our training is as simple as the below line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 400, batch_train_loss 2.4066, batch_train_acc 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 800, batch_train_loss 3.6834, batch_train_acc 0.359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 1200, batch_train_loss 3.4850, batch_train_acc 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 1600, batch_train_loss 2.3668, batch_train_acc 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 2000, batch_train_loss 2.0749, batch_train_acc 0.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 2400, batch_train_loss 1.8663, batch_train_acc 0.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 2800, batch_train_loss 3.2228, batch_train_acc 0.391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 2.7202, acc_train 0.556, F1_train 0.567, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t valid_loss 2.3670, acc_valid 0.600, F1_valid 0.610, \n",
      "time 170.62 sec\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 400, batch_train_loss 3.1705, batch_train_acc 0.391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 800, batch_train_loss 2.5148, batch_train_acc 0.562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 1200, batch_train_loss 2.5992, batch_train_acc 0.594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 1600, batch_train_loss 3.0123, batch_train_acc 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 2000, batch_train_loss 2.4197, batch_train_acc 0.484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 2400, batch_train_loss 2.3056, batch_train_acc 0.609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 2800, batch_train_loss 2.9305, batch_train_acc 0.469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, learning_rate 0.00100 \n",
      "\t train_loss 2.3764, acc_train 0.607, F1_train 0.618, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t valid_loss 2.3415, acc_valid 0.632, F1_valid 0.626, \n",
      "time 182.82 sec\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 400, batch_train_loss 2.5844, batch_train_acc 0.656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 800, batch_train_loss 2.4294, batch_train_acc 0.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 1200, batch_train_loss 2.0701, batch_train_acc 0.672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 1600, batch_train_loss 1.3226, batch_train_acc 0.766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 2000, batch_train_loss 2.9948, batch_train_acc 0.641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 2400, batch_train_loss 2.6953, batch_train_acc 0.609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 2800, batch_train_loss 2.4249, batch_train_acc 0.656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, learning_rate 0.00090 \n",
      "\t train_loss 2.2623, acc_train 0.627, F1_train 0.637, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t valid_loss 2.2170, acc_valid 0.654, F1_valid 0.658, \n",
      "time 183.85 sec\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, batch 400, batch_train_loss 2.1939, batch_train_acc 0.547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, batch 800, batch_train_loss 1.5434, batch_train_acc 0.725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, batch 1200, batch_train_loss 2.6781, batch_train_acc 0.578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, batch 1600, batch_train_loss 2.2670, batch_train_acc 0.609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, batch 2000, batch_train_loss 2.3136, batch_train_acc 0.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, batch 2400, batch_train_loss 1.9306, batch_train_acc 0.797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, batch 2800, batch_train_loss 1.6305, batch_train_acc 0.609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, learning_rate 0.00090 \n",
      "\t train_loss 2.1600, acc_train 0.643, F1_train 0.652, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t valid_loss 2.1611, acc_valid 0.652, F1_valid 0.657, \n",
      "time 183.27 sec\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, trainer, ctx, nepochs,\n",
    "            penal_coeff=penal_coeff, clip=clip, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and sampling using our model\n",
    "\n",
    "Now that the model has been trained, we can randomly input a sentence into the model and predict its emotional value tag. The range of emotional markers (or the labels) is one through five, each corresponding to the degree of negativity to positivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "\n",
      "[[[0.16078304 0.24099027 0.2368208  0.36140588]\n",
      "  [0.19309844 0.3100725  0.21409595 0.28273308]]]\n",
      "<NDArray 1x2x4 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "input_ar = nd.array(vocab[['This', 'movie', 'is', 'amazing']], ctx=ctx).reshape((1, -1))\n",
    "pred, att = model(input_ar)\n",
    "\n",
    "label = np.argmax(nd.softmax(pred, axis=1).asnumpy(), axis=1) + 1\n",
    "print(label)\n",
    "print(att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to intuitively understand the role of the attention mechanism, we visualize the output of the model's attention on the predicted samples using the `matplotlib` and `seaborn` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x72 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the attention layer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.squeeze(att.asnumpy(), 0).shape\n",
    "plt.figure(figsize=(8,1))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(np.squeeze(att.asnumpy(), 0), cmap=cmap, annot=True,\n",
    "            xticklabels=['This', 'movie', 'is', 'amazing'], yticklabels=['att0', 'att1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Word embedding can effectively represent the semantic similarity between words, which allows for many breakthroughs in complex natural language processing tasks. Attention mechanisms can intuitively grasp the important semantic features in the sentence. The LSTM captures the word-order relationship between words in a sentence. Through a combination of these three, word embeddings, LSTMs, and attention mechanisms, we can effectively represent the semantics of a sentence and apply it to many practical tasks.\n",
    "\n",
    "GluonNLP provides us with an efficient and convenient toolbox to help us experiment quickly. This greatly simplifies the tedious work of many natural language processing tasks.\n",
    "\n",
    "## References\n",
    "\n",
    "1. [A Structured Self-Attentive Sentence Embedding](https://arxiv.org/pdf/1703.03130.pdf)\n",
    "2. [Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "3. [Long short-term memory](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "4. [Skymind.AI A Beginner's Guide to Attention Mechanisms and Memory Networks](https://skymind.ai/wiki/attention-mechanism-memory-network)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}